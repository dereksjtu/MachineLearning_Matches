{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "t_login_path      = '../input/t_login.csv'\n",
    "t_login_test_path = '../input/t_login_test.csv'\n",
    "t_trade_path      = '../input/t_trade.csv'\n",
    "t_trade_test_path = '../input/t_trade_test.csv'\n",
    "t_trade2_path      = '../input/trade/trade_2_login.csv'\n",
    "t_trade2_test_login_path = '../input/trade/t_trade_test_2_login.csv'\n",
    "# t_login_train = pd.read_csv(t_login_path     )  \n",
    "# t_login_test  = pd.read_csv(t_login_test_path)\n",
    "# t_trade_train = pd.read_csv(t_trade_path     )\n",
    "# t_trade_test  = pd.read_csv(t_trade_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#log_id,id转换为字符串\n",
    "loginData=pd.read_csv(t_login_path,dtype={'log_id':str,'id':str})\n",
    "loginTestData=pd.read_csv(t_login_test_path,dtype={'log_id':str,'id':str})\n",
    "tradeData=pd.read_csv(t_trade2_path,dtype={'id':str,'login_id':str})\n",
    "tradeTestData=pd.read_csv(t_trade2_test_login_path,dtype={'id':str,'login_id':str})\n",
    "loginTestData=loginTestData.append(loginData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowkey</th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>is_risk</th>\n",
       "      <th>login_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-01 00:00:41.0</td>\n",
       "      <td>27863</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2015-01-01 00:00:46.0</td>\n",
       "      <td>115201</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>2015-01-01 00:01:02.0</td>\n",
       "      <td>143711</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>2015-01-01 00:03:00.0</td>\n",
       "      <td>172699</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>2015-01-01 00:03:52.0</td>\n",
       "      <td>53265</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>2015-01-01 00:05:51.0</td>\n",
       "      <td>18231</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94</td>\n",
       "      <td>2015-01-01 00:10:12.0</td>\n",
       "      <td>147174</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>98</td>\n",
       "      <td>2015-01-01 00:10:48.0</td>\n",
       "      <td>141674</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>116</td>\n",
       "      <td>2015-01-01 00:13:16.0</td>\n",
       "      <td>105324</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>147</td>\n",
       "      <td>2015-01-01 00:19:07.0</td>\n",
       "      <td>168749</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>198</td>\n",
       "      <td>2015-01-01 00:25:51.0</td>\n",
       "      <td>139034</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>282</td>\n",
       "      <td>2015-01-01 00:48:11.0</td>\n",
       "      <td>157472</td>\n",
       "      <td>0</td>\n",
       "      <td>8073763129665082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>287</td>\n",
       "      <td>2015-01-01 00:49:40.0</td>\n",
       "      <td>144195</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>293</td>\n",
       "      <td>2015-01-01 00:50:41.0</td>\n",
       "      <td>164198</td>\n",
       "      <td>0</td>\n",
       "      <td>666123372684626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>320</td>\n",
       "      <td>2015-01-01 00:58:28.0</td>\n",
       "      <td>115201</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>343</td>\n",
       "      <td>2015-01-01 01:05:47.0</td>\n",
       "      <td>87482</td>\n",
       "      <td>0</td>\n",
       "      <td>710878559931975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>344</td>\n",
       "      <td>2015-01-01 01:05:55.0</td>\n",
       "      <td>64543</td>\n",
       "      <td>0</td>\n",
       "      <td>5972875147058808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>380</td>\n",
       "      <td>2015-01-01 01:17:51.0</td>\n",
       "      <td>53933</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>393</td>\n",
       "      <td>2015-01-01 01:22:20.0</td>\n",
       "      <td>174789</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>419</td>\n",
       "      <td>2015-01-01 01:28:08.0</td>\n",
       "      <td>162466</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>440</td>\n",
       "      <td>2015-01-01 01:40:30.0</td>\n",
       "      <td>24002</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>464</td>\n",
       "      <td>2015-01-01 01:48:43.0</td>\n",
       "      <td>59616</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>472</td>\n",
       "      <td>2015-01-01 01:51:10.0</td>\n",
       "      <td>120026</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>476</td>\n",
       "      <td>2015-01-01 01:52:28.0</td>\n",
       "      <td>120026</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>478</td>\n",
       "      <td>2015-01-01 01:52:39.0</td>\n",
       "      <td>30357</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>503</td>\n",
       "      <td>2015-01-01 02:06:44.0</td>\n",
       "      <td>48831</td>\n",
       "      <td>0</td>\n",
       "      <td>8429434754433884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>506</td>\n",
       "      <td>2015-01-01 02:08:51.0</td>\n",
       "      <td>61579</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>528</td>\n",
       "      <td>2015-01-01 02:33:32.0</td>\n",
       "      <td>85435</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>536</td>\n",
       "      <td>2015-01-01 02:39:07.0</td>\n",
       "      <td>72922</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>555</td>\n",
       "      <td>2015-01-01 03:02:01.0</td>\n",
       "      <td>18138</td>\n",
       "      <td>1</td>\n",
       "      <td>5175043253945576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132689</th>\n",
       "      <td>950365</td>\n",
       "      <td>2015-06-30 22:31:39.0</td>\n",
       "      <td>168845</td>\n",
       "      <td>0</td>\n",
       "      <td>32342634235265133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132690</th>\n",
       "      <td>950366</td>\n",
       "      <td>2015-06-30 22:31:46.0</td>\n",
       "      <td>34934</td>\n",
       "      <td>0</td>\n",
       "      <td>0009129903077470525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132691</th>\n",
       "      <td>950387</td>\n",
       "      <td>2015-06-30 22:35:40.0</td>\n",
       "      <td>116080</td>\n",
       "      <td>0</td>\n",
       "      <td>4659248884217494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132692</th>\n",
       "      <td>950401</td>\n",
       "      <td>2015-06-30 22:37:52.0</td>\n",
       "      <td>64345</td>\n",
       "      <td>0</td>\n",
       "      <td>6530917879889501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>950418</td>\n",
       "      <td>2015-06-30 22:40:05.0</td>\n",
       "      <td>69954</td>\n",
       "      <td>0</td>\n",
       "      <td>8921058969001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132694</th>\n",
       "      <td>950433</td>\n",
       "      <td>2015-06-30 22:43:32.0</td>\n",
       "      <td>158245</td>\n",
       "      <td>0</td>\n",
       "      <td>29552109329278253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132695</th>\n",
       "      <td>950461</td>\n",
       "      <td>2015-06-30 22:49:28.0</td>\n",
       "      <td>78179</td>\n",
       "      <td>0</td>\n",
       "      <td>9064315508531161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132696</th>\n",
       "      <td>950495</td>\n",
       "      <td>2015-06-30 22:55:56.0</td>\n",
       "      <td>159008</td>\n",
       "      <td>0</td>\n",
       "      <td>2571418350663919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132697</th>\n",
       "      <td>950512</td>\n",
       "      <td>2015-06-30 22:58:03.0</td>\n",
       "      <td>32497</td>\n",
       "      <td>0</td>\n",
       "      <td>7029858065487775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132698</th>\n",
       "      <td>950527</td>\n",
       "      <td>2015-06-30 22:59:17.0</td>\n",
       "      <td>65838</td>\n",
       "      <td>0</td>\n",
       "      <td>0026240628001990007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132699</th>\n",
       "      <td>950532</td>\n",
       "      <td>2015-06-30 23:00:55.0</td>\n",
       "      <td>48280</td>\n",
       "      <td>0</td>\n",
       "      <td>857385525204737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132700</th>\n",
       "      <td>950534</td>\n",
       "      <td>2015-06-30 23:01:41.0</td>\n",
       "      <td>15112</td>\n",
       "      <td>0</td>\n",
       "      <td>10043019497288075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132701</th>\n",
       "      <td>950536</td>\n",
       "      <td>2015-06-30 23:02:27.0</td>\n",
       "      <td>57462</td>\n",
       "      <td>0</td>\n",
       "      <td>31514058917187016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132702</th>\n",
       "      <td>950559</td>\n",
       "      <td>2015-06-30 23:09:30.0</td>\n",
       "      <td>57462</td>\n",
       "      <td>0</td>\n",
       "      <td>31514058917187016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132703</th>\n",
       "      <td>950565</td>\n",
       "      <td>2015-06-30 23:10:44.0</td>\n",
       "      <td>119417</td>\n",
       "      <td>0</td>\n",
       "      <td>42486898146758945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132704</th>\n",
       "      <td>950573</td>\n",
       "      <td>2015-06-30 23:14:41.0</td>\n",
       "      <td>82440</td>\n",
       "      <td>0</td>\n",
       "      <td>49307410266542095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132705</th>\n",
       "      <td>950585</td>\n",
       "      <td>2015-06-30 23:17:56.0</td>\n",
       "      <td>152510</td>\n",
       "      <td>0</td>\n",
       "      <td>06438580819304063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132706</th>\n",
       "      <td>950587</td>\n",
       "      <td>2015-06-30 23:18:12.0</td>\n",
       "      <td>152510</td>\n",
       "      <td>0</td>\n",
       "      <td>06438580819304063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132707</th>\n",
       "      <td>950599</td>\n",
       "      <td>2015-06-30 23:19:17.0</td>\n",
       "      <td>70824</td>\n",
       "      <td>0</td>\n",
       "      <td>42645546696757397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132708</th>\n",
       "      <td>950615</td>\n",
       "      <td>2015-06-30 23:21:39.0</td>\n",
       "      <td>144435</td>\n",
       "      <td>0</td>\n",
       "      <td>8075489787058088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132709</th>\n",
       "      <td>950648</td>\n",
       "      <td>2015-06-30 23:32:04.0</td>\n",
       "      <td>92497</td>\n",
       "      <td>0</td>\n",
       "      <td>5145897545479122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132710</th>\n",
       "      <td>950654</td>\n",
       "      <td>2015-06-30 23:32:48.0</td>\n",
       "      <td>82440</td>\n",
       "      <td>0</td>\n",
       "      <td>49307410266542095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132711</th>\n",
       "      <td>950664</td>\n",
       "      <td>2015-06-30 23:35:42.0</td>\n",
       "      <td>93970</td>\n",
       "      <td>0</td>\n",
       "      <td>25266192668756204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132712</th>\n",
       "      <td>950665</td>\n",
       "      <td>2015-06-30 23:35:43.0</td>\n",
       "      <td>93970</td>\n",
       "      <td>0</td>\n",
       "      <td>25266192668756204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132713</th>\n",
       "      <td>950667</td>\n",
       "      <td>2015-06-30 23:36:07.0</td>\n",
       "      <td>49360</td>\n",
       "      <td>0</td>\n",
       "      <td>2756861225960412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132714</th>\n",
       "      <td>950697</td>\n",
       "      <td>2015-06-30 23:45:17.0</td>\n",
       "      <td>39176</td>\n",
       "      <td>0</td>\n",
       "      <td>3174092056027499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132715</th>\n",
       "      <td>950703</td>\n",
       "      <td>2015-06-30 23:46:07.0</td>\n",
       "      <td>61280</td>\n",
       "      <td>0</td>\n",
       "      <td>763439580729896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132716</th>\n",
       "      <td>950714</td>\n",
       "      <td>2015-06-30 23:49:32.0</td>\n",
       "      <td>72429</td>\n",
       "      <td>0</td>\n",
       "      <td>4665831126357872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132717</th>\n",
       "      <td>950720</td>\n",
       "      <td>2015-06-30 23:51:08.0</td>\n",
       "      <td>106040</td>\n",
       "      <td>0</td>\n",
       "      <td>06025337555329402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132718</th>\n",
       "      <td>950727</td>\n",
       "      <td>2015-06-30 23:53:19.0</td>\n",
       "      <td>72429</td>\n",
       "      <td>0</td>\n",
       "      <td>4665831126357872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132719 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rowkey                   time      id  is_risk             login_id\n",
       "0            3  2015-01-01 00:00:41.0   27863        0                  NaN\n",
       "1            6  2015-01-01 00:00:46.0  115201        0                  NaN\n",
       "2           13  2015-01-01 00:01:02.0  143711        0                  NaN\n",
       "3           32  2015-01-01 00:03:00.0  172699        0                  NaN\n",
       "4           43  2015-01-01 00:03:52.0   53265        0                  NaN\n",
       "5           67  2015-01-01 00:05:51.0   18231        0                  NaN\n",
       "6           94  2015-01-01 00:10:12.0  147174        0                  NaN\n",
       "7           98  2015-01-01 00:10:48.0  141674        0                  NaN\n",
       "8          116  2015-01-01 00:13:16.0  105324        0                  NaN\n",
       "9          147  2015-01-01 00:19:07.0  168749        0                  NaN\n",
       "10         198  2015-01-01 00:25:51.0  139034        0                  NaN\n",
       "11         282  2015-01-01 00:48:11.0  157472        0     8073763129665082\n",
       "12         287  2015-01-01 00:49:40.0  144195        0                  NaN\n",
       "13         293  2015-01-01 00:50:41.0  164198        0      666123372684626\n",
       "14         320  2015-01-01 00:58:28.0  115201        0                  NaN\n",
       "15         343  2015-01-01 01:05:47.0   87482        0      710878559931975\n",
       "16         344  2015-01-01 01:05:55.0   64543        0     5972875147058808\n",
       "17         380  2015-01-01 01:17:51.0   53933        0                  NaN\n",
       "18         393  2015-01-01 01:22:20.0  174789        0                  NaN\n",
       "19         419  2015-01-01 01:28:08.0  162466        0                  NaN\n",
       "20         440  2015-01-01 01:40:30.0   24002        0                  NaN\n",
       "21         464  2015-01-01 01:48:43.0   59616        0                  NaN\n",
       "22         472  2015-01-01 01:51:10.0  120026        0                  NaN\n",
       "23         476  2015-01-01 01:52:28.0  120026        0                  NaN\n",
       "24         478  2015-01-01 01:52:39.0   30357        0                  NaN\n",
       "25         503  2015-01-01 02:06:44.0   48831        0     8429434754433884\n",
       "26         506  2015-01-01 02:08:51.0   61579        0                  NaN\n",
       "27         528  2015-01-01 02:33:32.0   85435        0                  NaN\n",
       "28         536  2015-01-01 02:39:07.0   72922        0                  NaN\n",
       "29         555  2015-01-01 03:02:01.0   18138        1     5175043253945576\n",
       "...        ...                    ...     ...      ...                  ...\n",
       "132689  950365  2015-06-30 22:31:39.0  168845        0    32342634235265133\n",
       "132690  950366  2015-06-30 22:31:46.0   34934        0  0009129903077470525\n",
       "132691  950387  2015-06-30 22:35:40.0  116080        0     4659248884217494\n",
       "132692  950401  2015-06-30 22:37:52.0   64345        0     6530917879889501\n",
       "132693  950418  2015-06-30 22:40:05.0   69954        0     8921058969001608\n",
       "132694  950433  2015-06-30 22:43:32.0  158245        0    29552109329278253\n",
       "132695  950461  2015-06-30 22:49:28.0   78179        0     9064315508531161\n",
       "132696  950495  2015-06-30 22:55:56.0  159008        0     2571418350663919\n",
       "132697  950512  2015-06-30 22:58:03.0   32497        0     7029858065487775\n",
       "132698  950527  2015-06-30 22:59:17.0   65838        0  0026240628001990007\n",
       "132699  950532  2015-06-30 23:00:55.0   48280        0      857385525204737\n",
       "132700  950534  2015-06-30 23:01:41.0   15112        0    10043019497288075\n",
       "132701  950536  2015-06-30 23:02:27.0   57462        0    31514058917187016\n",
       "132702  950559  2015-06-30 23:09:30.0   57462        0    31514058917187016\n",
       "132703  950565  2015-06-30 23:10:44.0  119417        0    42486898146758945\n",
       "132704  950573  2015-06-30 23:14:41.0   82440        0    49307410266542095\n",
       "132705  950585  2015-06-30 23:17:56.0  152510        0    06438580819304063\n",
       "132706  950587  2015-06-30 23:18:12.0  152510        0    06438580819304063\n",
       "132707  950599  2015-06-30 23:19:17.0   70824        0    42645546696757397\n",
       "132708  950615  2015-06-30 23:21:39.0  144435        0     8075489787058088\n",
       "132709  950648  2015-06-30 23:32:04.0   92497        0     5145897545479122\n",
       "132710  950654  2015-06-30 23:32:48.0   82440        0    49307410266542095\n",
       "132711  950664  2015-06-30 23:35:42.0   93970        0    25266192668756204\n",
       "132712  950665  2015-06-30 23:35:43.0   93970        0    25266192668756204\n",
       "132713  950667  2015-06-30 23:36:07.0   49360        0     2756861225960412\n",
       "132714  950697  2015-06-30 23:45:17.0   39176        0     3174092056027499\n",
       "132715  950703  2015-06-30 23:46:07.0   61280        0      763439580729896\n",
       "132716  950714  2015-06-30 23:49:32.0   72429        0     4665831126357872\n",
       "132717  950720  2015-06-30 23:51:08.0  106040        0    06025337555329402\n",
       "132718  950727  2015-06-30 23:53:19.0   72429        0     4665831126357872\n",
       "\n",
       "[132719 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>timelong</th>\n",
       "      <th>device</th>\n",
       "      <th>log_from</th>\n",
       "      <th>ip</th>\n",
       "      <th>city</th>\n",
       "      <th>result</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>is_scan</th>\n",
       "      <th>is_sec</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [log_id, timelong, device, log_from, ip, city, result, timestamp, type, id, is_scan, is_sec, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loginData[loginData['id'] == '157472']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到DataFrame中的无重复的id\n",
    "def getUserIDFromDataFrame(dataFrame):\n",
    "    return pd.DataFrame({'id':dataFrame['id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到登录中所用时间为整秒的计算\n",
    "def getTimelongScale(loginData,test):\n",
    "    #计算是否为整秒\n",
    "    loginData['timeScale']=(loginData['timelong']%1000==0)\n",
    "    #分别为整数 ，不为整数的次数\n",
    "    gd=loginData['timeScale'].groupby([loginData['id'],loginData['timeScale']])\n",
    "    tmp=gd.count().unstack().reset_index()\n",
    "    tmp=tmp.fillna(0)\n",
    "    test=pd.merge(tmp,test,on='id',how='inner')\n",
    "#     print(test.head(2))\n",
    "    del loginData['timeScale'],tmp\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组不同的个数\n",
    "def getCountsByColumnName(loginData,test,columnName):\n",
    "    num=loginData[['id',columnName]].groupby(loginData['id'])[columnName].nunique()\n",
    "    t=pd.DataFrame(num)\n",
    "    t.insert(0, 'id', num.index)\n",
    "    t.columns=['id',columnName+'_Counts']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组平均值\n",
    "def getMeanByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.mean().reset_index()\n",
    "    t.columns=['id',columnName+'_Mean']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组最小值\n",
    "def getMinByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.min().reset_index()\n",
    "    t.columns=['id',columnName+'_min']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组最大值\n",
    "def getMaxByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.max().reset_index()\n",
    "    t.columns=['id',columnName+'_max']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组方差\n",
    "def getVarByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.var().reset_index()\n",
    "    t.columns=['id',columnName+'_var']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到某列的分组标准差\n",
    "def getStdByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.std().reset_index()\n",
    "    t.columns=['id',columnName+'_std']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到对登录result的处理\n",
    "def getLoginResultCount(loginData,test):\n",
    "    r31=loginData[loginData['result']!=1]\n",
    "    t=r31['result'].groupby(r31['id'])\n",
    "    t=t.count().reset_index()\n",
    "    t.columns=['id','result_no1_Counts']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    test=test.fillna(0)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#处理交易数据\n",
    "def tradeDataInit(tradeData,loginData):\n",
    "    tradeData=tradeData.copy()\n",
    "    #构建一个小时的\n",
    "    tradeData['hours']=tradeData['time'].str.extract('\\\\s(\\\\d\\\\d):')\n",
    "    tradeData['hours']=tradeData['hours'].astype('int')\n",
    "    tradeData=pd.merge(tradeData,loginData[['log_id','time']],left_on='login_id',right_on='log_id',how='inner')\n",
    "    del tradeData['log_id']\n",
    "#     tradeData['hours']=\n",
    "#     print(OneHotEncoder(sparse = False).fit_transform( tradeData[['hours']]))\n",
    "    #交易时间\n",
    "    tradeData['tx']=pd.to_datetime(tradeData['time_x'])\n",
    "    #登录时间\n",
    "    tradeData['ty']=pd.to_datetime(tradeData['time_y'])\n",
    "    #交易时间和登录时间之间的差值\n",
    "    tradeData['st']=(tradeData['tx']-tradeData['ty']).dt.seconds\n",
    "    #每次交易时间的差\n",
    "    tradeData['trade_time_sub']=tradeData.sort_values(by='tx').groupby('id')['tx'].diff()\n",
    "    tradeData['trade_time_sub_day']=tradeData['trade_time_sub'].dt.days\n",
    "    tradeData=getMaxByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getMeanByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getMinByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getStdByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getVarByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    #使用的比例\n",
    "    tradeData=getTradeLoginColumScale(tradeData,loginData,'device')\n",
    "    tradeData=getTradeLoginColumScale(tradeData,loginData,'city')\n",
    "    \n",
    "    del tradeData['tx'],tradeData['ty'],tradeData['time_y'],tradeData['login_id'],tradeData['trade_time_sub']\n",
    "    tradeData.rename(columns={'time_x':'time'},inplace=True)\n",
    "    return tradeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTradeLoginColumScale(tradeData,loginData,cname):\n",
    "    #计算cname列上分别使用的个数\n",
    "    c_count=loginData[['id',cname,'log_id']].groupby(['id',cname]).count()\n",
    "    c_count=c_count.reset_index()\n",
    "    c_count.columns=['id',cname,cname+'_count_tmp']\n",
    "    #使用的总个数\n",
    "    c_sum=c_count[['id',cname+'_count_tmp']].groupby('id').sum().reset_index()\n",
    "    c_sum.columns=['id',cname+'_count_sum']\n",
    "    c=pd.merge(c_count,c_sum,on='id',how='inner')\n",
    "    #使用的比例\n",
    "    c[cname+'_scale']=c[cname+'_count_tmp']/c[cname+'_count_sum']\n",
    "    del c[cname+'_count_tmp'],c[cname+'_count_sum']\n",
    "    c=pd.merge(loginData[['log_id','id',cname]],c,on=['id',cname],how='inner')\n",
    "    tradeData=pd.merge(tradeData,c[['log_id',cname+'_scale']],left_on='login_id',right_on='log_id',how='inner')\n",
    "    del c,tradeData['log_id']\n",
    "    return tradeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#把处理后的登录数据和交易数据内连接\n",
    "def createAllData(test,tradeData):\n",
    "    tmp=pd.merge(test,tradeData,on='id',how='inner')\n",
    "#     tmp=tmp.fillna(0)\n",
    "#     print(tmp.info())\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "#评估函数\n",
    "def rocJdScore(*args):\n",
    "    from sklearn import metrics\n",
    "    return metrics.make_scorer(fbeta_score,beta=0.1, greater_is_better=True)(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成训练用的pipline\n",
    "def getPipe():\n",
    "    # 下面，我要用逻辑回归拟合模型，并用标准化和PCA（30维->2维）对数据预处理，用Pipeline类把这些过程链接在一起\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import xgboost as xgb\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "    #xgb的配置\n",
    "    xgbFier = XGBClassifier(\n",
    "             learning_rate =0.3,\n",
    "             n_estimators=1000,\n",
    "             max_depth=5,\n",
    "             min_child_weight=1,\n",
    "             gamma=0,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=0.8,\n",
    "             objective= 'binary:logistic',\n",
    "             nthread=2,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27,\n",
    "             silent=0\n",
    "    )\n",
    "    # 用StandardScaler和PCA作为转换器，LogisticRegression作为评估器\n",
    "    estimators = [\n",
    "#         ('scl', StandardScaler()), \n",
    "#                   ('pca', PCA(n_components=2)), \n",
    "#                    ('rf', RandomForestClassifier(random_state=1,\n",
    "#                                                  max_depth= 50,\n",
    "#                                                  min_samples_leaf= 3,\n",
    "#                                                  min_samples_split= 10,\n",
    "#                                                  n_estimators= 20,\n",
    "#                                                 )),\n",
    "#                   ('dtc',DecisionTreeClassifier(criterion='entropy')),\n",
    "                                    ('xgb',xgbFier),\n",
    "#                   ('lr', LogisticRegression())\n",
    "                 ]\n",
    "    # estimators = [ ('clf', RandomForestClassifier(random_state=1))]\n",
    "    # Pipeline类接收一个包含元组的列表作为参数，每个元组的第一个值为任意的字符串标识符，\n",
    "    #比如：我们可以通过pipe_lr.named_steps['pca']来访问PCA组件;第二个值为scikit-learn的转换器或评估器\n",
    "    pipe_lr = Pipeline(estimators)\n",
    "    return pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到训练用的测试集元组（x，y）\n",
    "def getTrainData(isUndersample=False):\n",
    "    allData=transferData(loginData,tradeData)\n",
    "    if(isUndersample):\n",
    "        #进行undersample的处理\n",
    "        number_records_fraud=len(allData[allData['is_risk']==1]) #有风险的个数\n",
    "        fraud_indices=np.array(allData[allData['is_risk']==1].index) #有风险的index\n",
    "        normal_indices=allData[allData['is_risk']==0].index\n",
    "        random_normal_indices=np.random.choice(normal_indices,number_records_fraud,replace=False)\n",
    "        random_normal_indices=np.array(random_normal_indices)\n",
    "        under_sample_indices=np.concatenate([fraud_indices,random_normal_indices])\n",
    "        allData=allData.iloc[under_sample_indices,:]\n",
    "    \n",
    "    x=allData.iloc[:,3:].values\n",
    "    y=allData['is_risk'].values\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成学习算法\n",
    "def jdPipeFited(pipe_lr):    \n",
    "    x,y=getTrainData(isUndersample=False)\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    # 拆分成训练集(80%)和测试集(20%)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1) \n",
    "    \n",
    "    pipe_lr.fit(x, y)\n",
    "    return pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "你可以理解为短时间内变化城市、切换IP、更换设备、三更半夜多笔交易、不在常用设备登陆、不在常在城市登陆、短时间多次登陆的后续交易\n",
    "'''\n",
    "#生成训练数据\n",
    "def transferData(loginData,tradeData):    \n",
    "    lData=getUserIDFromDataFrame(loginData)\n",
    "    #登录耗费时间段是否为整数\n",
    "    lData=getTimelongScale(loginData,lData)\n",
    "    lData=getMinByColumnName(loginData,lData,'timelong')\n",
    "    lData=getMaxByColumnName(loginData,lData,'timelong')\n",
    "    lData=getMeanByColumnName(loginData,lData,'timelong')\n",
    "    lData=getVarByColumnName(loginData,lData,'timelong')\n",
    "    lData=getStdByColumnName(loginData,lData,'timelong')\n",
    "# #     #device的个数\n",
    "    lData=getCountsByColumnName(loginData,lData,'device')\n",
    "# #     #ip的个数\n",
    "    lData=getCountsByColumnName(loginData,lData,'ip')\n",
    "    lData=getCountsByColumnName(loginData,lData,'log_from')\n",
    "    \n",
    "    lData=getCountsByColumnName(loginData,lData,'type')\n",
    "#     #城市的个数\n",
    "    lData=getCountsByColumnName(loginData,lData,'city')\n",
    "    #登录的次数\n",
    "    lData=getCountsByColumnName(loginData,lData,'log_id')\n",
    "    #平均值\n",
    "#     lData=getMeanByColumnName(loginData,lData,'device')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'log_from')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'ip')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'city')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'result')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'type')\n",
    "    #result的处理\n",
    "    lData=getLoginResultCount(loginData,lData)\n",
    "    \n",
    "    tData=tradeData.copy()\n",
    "    tData=tradeDataInit(tData,loginData)\n",
    "#     tData=getLastSubTime(loginData,tData)\n",
    "    allData=createAllData(lData,tData)\n",
    "    del allData['time']\n",
    "    # get a list of columns\n",
    "    cols = list(allData)\n",
    "    if 'is_risk' in allData.columns:\n",
    "        cols.insert(0, cols.pop(cols.index('is_risk')))\n",
    "    cols.insert(0, cols.pop(cols.index('rowkey')))\n",
    "    \n",
    "    allData = allData.ix[:, cols]\n",
    "    print(allData.head(2))\n",
    "    return allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rowkey  is_risk      id  result_no1_Counts  log_id_Counts  city_Counts  \\\n",
      "0  745277        0  100002                  4              9            2   \n",
      "1  745744        0  100002                  4              9            2   \n",
      "\n",
      "   type_Counts  log_from_Counts  ip_Counts  device_Counts     ...      \\\n",
      "0            2                2          3              3     ...       \n",
      "1            2                2          3              3     ...       \n",
      "\n",
      "   trade_time_sub_day_var  trade_time_sub_day_std  trade_time_sub_day_min  \\\n",
      "0                     NaN                     NaN                     0.0   \n",
      "1                     NaN                     NaN                     0.0   \n",
      "\n",
      "   trade_time_sub_day_Mean  trade_time_sub_day_max  hours    st  \\\n",
      "0                      0.0                     0.0     22  1688   \n",
      "1                      0.0                     0.0     23   234   \n",
      "\n",
      "   trade_time_sub_day  device_scale  city_scale  \n",
      "0                 NaN      0.333333    0.777778  \n",
      "1                 0.0      0.333333    0.777778  \n",
      "\n",
      "[2 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#k-fold交叉验证\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "pipe_lr=getPipe()\n",
    "X_train,y_train=getTrainData(isUndersample=False)\n",
    "#记录程序运行时间\n",
    "import time \n",
    "start_time = time.time()\n",
    "scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=2,scoring=rocJdScore)\n",
    "print(scores)\n",
    "#整体预测\n",
    "X_train,y_train=getTrainData(isUndersample=False)\n",
    "pipe_lr\n",
    "#输出运行时长\n",
    "cost_time = time.time()-start_time\n",
    "print(\"交叉验证 success!\",'\\n',\"cost time:\",cost_time,\"(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#网格搜索实验\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {\n",
    "#     'rf__n_estimators': (5, 10, 20, 50),\n",
    "#     'rf__max_depth': (50, 150, 250),\n",
    "#     'rf__min_samples_split': [10, 2, 3],\n",
    "#     'rf__min_samples_leaf': (1, 2, 3),\n",
    "    #xgb的参数\n",
    "    'xgb__max_depth':(4,6),\n",
    "    'xgb__learning_rate':(0.3,0.5)\n",
    "\n",
    "}\n",
    "pipe_lr=getPipe()\n",
    "X_train,y_train=getTrainData()\n",
    "\n",
    "\n",
    "#网格搜索\n",
    "grid_search = GridSearchCV(pipe_lr, parameters, n_jobs=-1, verbose=1, scoring=rocJdScore)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#获取最优参数\n",
    "print('最佳效果：%0.3f' % grid_search.best_score_)\n",
    "print('最优参数：')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s= %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# #预测以及分类器参数报告\n",
    "# predictions = grid_search.predict(X_test)\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#学习曲线\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "pipe_lr = getPipe()\n",
    "X_train,y_train=getTrainData()\n",
    "# train_sizes参数指定用于生成学习曲线的训练集数量，如果是分数指的是相对数量，整数指的是绝对数量\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10,\n",
    "                                                        n_jobs=2,scoring=rocJdScore)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#预测\n",
    "pipe=getPipe()\n",
    "pipe=jdPipeFited(pipe)\n",
    "preData=transferData(loginTestData,tradeTestData)\n",
    "x_pred=preData.iloc[:,2:].values\n",
    "y_pred=pipe.predict(x_pred)\n",
    "np.sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=pd.DataFrame(y_pred)\n",
    "subData=pd.DataFrame(preData['rowkey'])\n",
    "subData['is_risk']=p\n",
    "#之前用很多inner join，很多数据没有，都默认处理为没有风险\n",
    "subData=pd.merge(tradeTestData[['rowkey']],subData,on='rowkey',how='left')\n",
    "subData=subData.fillna(0)\n",
    "subData['is_risk']=subData['is_risk'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subData.to_csv('./sub.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
